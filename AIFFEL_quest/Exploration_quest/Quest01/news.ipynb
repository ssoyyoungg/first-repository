import nltk
nltk.download('stopwords')

import numpy as np
import pandas as pd
import os
import re
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from bs4 import BeautifulSoup 
from tensorflow.keras.preprocessing.text import Tokenizer 
from tensorflow.keras.preprocessing.sequence import pad_sequences
import urllib.request
import warnings
warnings.filterwarnings("ignore", category=UserWarning, module='bs4')
import urllib.request
urllib.request.urlretrieve("https://raw.githubusercontent.com/sunnysai12345/News_Summary/master/news_summary_more.csv", filename="news_summary_more.csv")
data = pd.read_csv('news_summary_more.csv', encoding='iso-8859-1')
data.sample(10)
## 전처리
### 중복 제거
print('text 열에서 중복을 배제한 유일한 샘플의 수 :', data['text'].nunique())
print('headlines 열에서 중복을 배제한 유일한 샘플의 수 :', data['headlines'].nunique())
len(data)
data.drop_duplicates(subset = ['text'], inplace=True)
print('전체 샘플수 :', (len(data)))
### Null 값 제거
print(data.isnull().sum())
# data.dropna(axis=0, inplace=True)
### 텍스트 정규화 및 불용어 제거
#### 정규화를 위한 사전
# 노드학습에서 사용했던 arturomp의 정규화 사전 (https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python)
contractions = {"ain't": "is not", "aren't": "are not","can't": "cannot", "'cause": "because", "could've": "could have", "couldn't": "could not",
                           "didn't": "did not",  "doesn't": "does not", "don't": "do not", "hadn't": "had not", "hasn't": "has not", "haven't": "have not",
                           "he'd": "he would","he'll": "he will", "he's": "he is", "how'd": "how did", "how'd'y": "how do you", "how'll": "how will", "how's": "how is",
                           "I'd": "I would", "I'd've": "I would have", "I'll": "I will", "I'll've": "I will have","I'm": "I am", "I've": "I have", "i'd": "i would",
                           "i'd've": "i would have", "i'll": "i will",  "i'll've": "i will have","i'm": "i am", "i've": "i have", "isn't": "is not", "it'd": "it would",
                           "it'd've": "it would have", "it'll": "it will", "it'll've": "it will have","it's": "it is", "let's": "let us", "ma'am": "madam",
                           "mayn't": "may not", "might've": "might have","mightn't": "might not","mightn't've": "might not have", "must've": "must have",
                           "mustn't": "must not", "mustn't've": "must not have", "needn't": "need not", "needn't've": "need not have","o'clock": "of the clock",
                           "oughtn't": "ought not", "oughtn't've": "ought not have", "shan't": "shall not", "sha'n't": "shall not", "shan't've": "shall not have",
                           "she'd": "she would", "she'd've": "she would have", "she'll": "she will", "she'll've": "she will have", "she's": "she is",
                           "should've": "should have", "shouldn't": "should not", "shouldn't've": "should not have", "so've": "so have","so's": "so as",
                           "this's": "this is","that'd": "that would", "that'd've": "that would have", "that's": "that is", "there'd": "there would",
                           "there'd've": "there would have", "there's": "there is", "here's": "here is","they'd": "they would", "they'd've": "they would have",
                           "they'll": "they will", "they'll've": "they will have", "they're": "they are", "they've": "they have", "to've": "to have",
                           "wasn't": "was not", "we'd": "we would", "we'd've": "we would have", "we'll": "we will", "we'll've": "we will have", "we're": "we are",
                           "we've": "we have", "weren't": "were not", "what'll": "what will", "what'll've": "what will have", "what're": "what are",
                           "what's": "what is", "what've": "what have", "when's": "when is", "when've": "when have", "where'd": "where did", "where's": "where is",
                           "where've": "where have", "who'll": "who will", "who'll've": "who will have", "who's": "who is", "who've": "who have",
                           "why's": "why is", "why've": "why have", "will've": "will have", "won't": "will not", "won't've": "will not have",
                           "would've": "would have", "wouldn't": "would not", "wouldn't've": "would not have", "y'all": "you all",
                           "y'all'd": "you all would","y'all'd've": "you all would have","y'all're": "you all are","y'all've": "you all have",
                           "you'd": "you would", "you'd've": "you would have", "you'll": "you will", "you'll've": "you will have",
                           "you're": "you are", "you've": "you have"}

print("정규화 사전의 수: ", len(contractions))
#### 불용어 제거 대비 불용어 확인
print('불용어 개수 :', len(stopwords.words('english') ))
print(stopwords.words('english'))
#### 텍스트 정규화와 불용어 및 기타 전처리를 적용하는 함수 정의
def preprocess_sentence(sentence, remove_stopwords=True):
    # 소문자화, 태그 제거, 괄호 내용 생략, 따옴표 제거, 소유격 제거, 숫자 및 특수문자 공백화, 반복 철자 줄임
    sentence = sentence.lower()
    sentence = BeautifulSoup(sentence, "lxml").text
    sentence = re.sub(r'\([^)]*\)', '', sentence)
    sentence = re.sub('"','', sentence)
    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(" ")])
    sentence = re.sub(r"'s\b","", sentence)
    sentence = re.sub("[^a-zA-Z]", " ", sentence)
    sentence = re.sub('[m]{2,}', 'mm', sentence)
    
    # 불용어 제거 (Text)
    if remove_stopwords:
        tokens = ' '.join(word for word in sentence.split() if not word in stopwords.words('english') if len(word) > 1)
    # 불용어 미제거 (Summary)
    else:
        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)
    return tokens
#### 훈련 데이터셋에 수행
clean_text = []
for line in data["text"]:
    clean_text.append(preprocess_sentence(line))

print("Text 전처리 전 원문: ", data["text"][:5])
print("Text 전처리 후 결과: ", clean_text[:5])
clean_headlines = []
for line in data["headlines"]:
    clean_headlines.append(preprocess_sentence(line, False)) # 불용어 제거하지 않음

# 전처리 후 출력
print("Text 전처리 전 원문: ", data["headlines"][:5])
print("Summary 전처리 후 결과: ", clean_headlines[:5])
#### 정제 후 샘플에 대입시키고, 빈 샘플 확인
data['text'] = clean_text
data['headlines'] = clean_headlines

# 빈 값을 Null 값으로 변환
data.replace('', np.nan, inplace=True)

data.isnull().sum()
### 샘플 크기 결정하기
#### 시각화로 분포 확인
import matplotlib.pyplot as plt

text_len = [len(s.split()) for s in data['text']]
headlines_len = [len(s.split()) for s in data['headlines']]

print('텍스트의 최소 길이 : {}'.format(np.min(text_len)))
print('텍스트의 최대 길이 : {}'.format(np.max(text_len)))
print('텍스트의 평균 길이 : {}'.format(np.mean(text_len)))
print('요약의 최소 길이 : {}'.format(np.min(headlines_len)))
print('요약의 최대 길이 : {}'.format(np.max(headlines_len)))
print('요약의 평균 길이 : {}'.format(np.mean(headlines_len)))
plt.subplot(1,2,1)
plt.boxplot(text_len)
plt.title('text')
plt.subplot(1,2,2)
plt.boxplot(headlines_len)
plt.title('headlines')
plt.tight_layout()
plt.show()

plt.title('text')
plt.hist(text_len, bins = 100)
plt.xlabel('length of samples')
plt.ylabel('number of samples')
plt.show()

plt.title('headlines')
plt.hist(headlines_len, bins = 100)
plt.xlabel('length of samples')
plt.ylabel('number of samples')
plt.show()
#### 샘플 최대최소 길이 조정
text_min_len = 20
text_max_len = 50
headlines_min_len = 5
headlines_max_len = 14
def within_threshold_len(min_len, max_len, nested_list):
    cnt = 0
    for s in nested_list:
        if(min_len <= len(s.split()) <= max_len):
            cnt = cnt + 1
    print('전체 샘플 중 길이가 %s 이상 %s 이하인 샘플의 비율: %s' % (min_len, max_len, (cnt / len(nested_list))))
# 텍스트와 요약문에 대해 min_len과 max_len을 함께 사용
within_threshold_len(text_min_len, text_max_len, data['text'])
within_threshold_len(headlines_min_len, headlines_max_len, data['headlines'])

# 데이터프레임 필터링: 텍스트와 요약문 모두 주어진 길이 범위 내에 있는 샘플만 남김
data = data[(data['text'].apply(lambda x: text_min_len <= len(x.split()) <= text_max_len)) &
            (data['headlines'].apply(lambda x: headlines_min_len <= len(x.split()) <= headlines_max_len))]
text_min_len2 = 32
text_max_len2 = 39
headlines_min_len2 = 7
headlines_max_len2 = 12
within_threshold_len(text_min_len2, text_max_len2, data['text'])
within_threshold_len(headlines_min_len2, headlines_max_len2, data['headlines'])

# 데이터프레임 필터링: 텍스트와 요약문 모두 주어진 길이 범위 내에 있는 샘플만 남김
data2 = data[(data['text'].apply(lambda x: text_min_len2 <= len(x.split()) <= text_max_len2)) &
            (data['headlines'].apply(lambda x: headlines_min_len2 <= len(x.split()) <= headlines_max_len2))]
len(data2)
len(data)
# 저장해두기
data_saved = data
# 편의상
data = data2
#### SOS, EOS 토큰 추가
data['decoder_input'] = data['headlines'].apply(lambda x : 'sostoken '+ x)
data['decoder_target'] = data['headlines'].apply(lambda x : x + ' eostoken')
data.head()
